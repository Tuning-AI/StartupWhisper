{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuDgwR4x3Anj"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 sentence_transformers langchain tiktoken duckduckgo-search xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login --token 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"
      ],
      "metadata": {
        "id": "iLo_t_p-3v7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM , BitsAndBytesConfig\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import torch"
      ],
      "metadata": {
        "id": "_FWtiqZkNAH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
        "    bnb_4bit_use_double_quant=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": 0})\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model = PeftModel.from_pretrained(model, \"TuningAI/Llama2_13B_startup_Assistant\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\" , trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_length=2048,temperature=0.5,top_p=0.95,repetition_penalty=1.15)\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "XPwondP-3FRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "chat = ConversationChain(\n",
        "    llm=local_llm,\n",
        "    verbose=False ,\n",
        "    memory=memory\n",
        ")\n",
        "chat.prompt.template = \\\n",
        "\"\"\"\n",
        "### INPUT:\n",
        "You will provide a detailed response to a user's inquiry about startups.\n",
        "Previous Conversation :\n",
        "{history}\n",
        "\n",
        "Current conversation:\n",
        "### INPUT: {input}\n",
        "### OUTPUT:\"\"\""
      ],
      "metadata": {
        "id": "c9DZzvho4lrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while 1 :\n",
        "  input_text = input(\">>\")\n",
        "  print(chat.predict(input=str(input_text)))"
      ],
      "metadata": {
        "id": "cdXCmXn45_gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}